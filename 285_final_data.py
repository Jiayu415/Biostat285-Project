# -*- coding: utf-8 -*-
"""285 Final data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19JRNrpuTpdaXt1ujPdyDsEnbuF_tXVkD
"""

import pandas as pd
import numpy as np

mimic_path = "/Users/gaojiayu/Desktop/mimic-iv-2.2/hosp"

admissions = pd.read_csv('admissions.csv.gz')

patients = pd.read_csv('patients.csv.gz')

diagnoses_icd = pd.read_csv('d_icd_diagnoses.csv.gz')

procedures_icd = pd.read_csv('d_icd_procedures.csv.gz')

chartevents = pd.read_csv('filtered_chartevents_new.csv')

labevents = pd.read_csv('labevents.csv.gz')

lab_items_hosp = pd.read_csv('d_labitems.csv.gz')

print(lab_items)

items_icu = pd.read_csv('d_items.csv')
print(items_icu)

prescriptions = pd.read_csv('prescriptions.csv.gz')

import pandas as pd

# Define the processing function (adjust based on your needs)
def process(chunk):
    # For example, just print the first few rows of each chunk
    print(chunk.head())
    # Add your specific processing logic here, e.g., filtering, analysis, etc.

# Set the chunk size
chunk_size = 500000

# Iterate over the file in chunks and process each chunk
for chunk in pd.read_csv('labevents.csv.gz', chunksize=chunk_size):
    process(chunk)



print(diagnoses_icd.columns)

print(patients.columns)



"""# Extract patients who has those chronic disease

# Extract patients with Chronic Disease & demographic & admission info
"""

import pandas as pd
import itertools

# Define ICD codes for chronic diseases
chronic_icd_codes = {
    "Diabetes": ["250.*", "E11.*"],
    "Kidney Disease": ["585.*", "N18.*"],
    "Depression": ["296.*", "F32.*", "F33.*"],
    "Arthritis": ["715.*", "M15.*"],
    "Stroke": ["434.*", "I63.*"]
}

# Flatten the chronic_icd_codes dictionary into a list of ICD codes
icd_codes_flat = list(itertools.chain(*chronic_icd_codes.values()))

# Assuming diagnoses_icd contains the 'icd_code' and 'subject_id'
# Merge diagnoses_icd with patients to get subject_id
diagnoses_with_subjects = pd.merge(diagnoses_icd, patients[['subject_id']], left_index=True, right_index=True, how='inner')

# Filter diagnoses_icd for chronic diseases using regex match
chronic_patients = diagnoses_with_subjects[
    diagnoses_with_subjects['icd_code'].str.contains('|'.join(icd_codes_flat), regex=True)
].copy()  # Create a copy to avoid the SettingWithCopyWarning

# Add a new column 'chronic_disease' to indicate the disease for each patient
def map_chronic_disease(icd_code):
    for disease, codes in chronic_icd_codes.items():
        for code in codes:
            if pd.Series(icd_code).str.contains(code).any():  # Check if ICD code matches
                return disease
    return "Other"  # Return 'Other' if no match is found

chronic_patients['chronic_disease'] = chronic_patients['icd_code'].apply(map_chronic_disease)

# Now filter the patients DataFrame to get the demographic information of chronic patients
chronic_patient_ids = chronic_patients['subject_id'].unique()

# Filter the patients DataFrame to get the demographic information
chronic_patient_data = patients[patients['subject_id'].isin(chronic_patient_ids)]

# Merge the chronic patient demographic data with the chronic disease info
chronic_patient_data_with_disease = pd.merge(
    chronic_patient_data,
    chronic_patients[['subject_id', 'chronic_disease']],
    on='subject_id',
    how='inner'
)

# Merge the chronic_patient_data_with_disease with the admissions DataFrame to include 'race' and 'insurance'
chronic_patient_data_with_race_and_insurance = pd.merge(
    chronic_patient_data_with_disease,
    admissions[['subject_id', 'race', 'insurance']],  # Include race and insurance columns from admissions
    on='subject_id',
    how='inner'
)

# Display the final DataFrame with chronic disease names, demographics, race, and insurance
print(chronic_patient_data_with_race_and_insurance[['subject_id', 'gender', 'race', 'insurance', 'anchor_age', 'chronic_disease']].head())

num_patients = chronic_patient_data_with_disease.shape[0]
print(f"Number of patients: {num_patients}")

# Save the final dataset as a CSV file
chronic_patient_data_with_race_and_insurance.to_csv('chronic_patient_data_with_race_and_insurance.csv', index=False)



"""# Extract Vitals and Lab Measurements"""

merged_data = pd.merge(
    chartevents,
    chronic_patient_data_with_race_and_insurance,
    on='subject_id',
    how='inner'
)

# Check the first few rows of the merged dataset
print(merged_data.head())

# Convert itemid to integer in both datasets
merged_data['itemid'] = merged_data['itemid'].astype('Int64')
lab_items['itemid'] = lab_items['itemid'].astype('Int64')

print(merged_data)

# Drop 'merged_label' column if it's not needed
merged_data_cleaned = merged_data.drop(columns=['merged_label'])

# Check the cleaned data
print(merged_data_cleaned.head())

# Convert itemid to integer type (or object, depending on your preference)
merged_data['itemid'] = merged_data['itemid'].astype(int)  # Convert to int64
items_icu['itemid'] = items_icu['itemid'].astype(int)  # Convert to int64

# Merge chart events (items) with merged_data on itemid
merged_data_with_items = pd.merge(
    merged_data,
    items_icu[['itemid', 'label']],  # Select relevant columns from items
    on='itemid',  # Merge on itemid
    how='left',  # Left join to keep all records from merged_data
    suffixes=('', '_chart')  # Add suffixes to differentiate label columns
)

# Check the result
print(merged_data_with_items.head())

# Merge lab events (labitems) with merged_data_with_items on itemid
merged_data_with_labitems = pd.merge(
    merged_data_with_items,
    lab_items_hosp[['itemid', 'label']],  # Select relevant columns from labitems
    on='itemid',  # Merge on itemid
    how='left',  # Left join to keep all records from merged_data_with_items
    suffixes=('_chart', '_lab')  # Add suffixes to differentiate label columns
)

# Check the result
print(merged_data_with_labitems.head())

# Merge lab events (labitems) with merged_data_with_items on itemid
merged_data_with_labitems = pd.merge(
  merged_data_with_items,  # The data frame with chart event details
    lab_items_hosp[['itemid', 'label']],  # Include additional columns
    on='itemid',  # Merge based on itemid
    how='left',  # Left join to keep all records from merged_data_with_items
    suffixes=('_chart', '_lab')  # Add suffixes to differentiate label columns
)

# Check the result
print(merged_data_with_labitems[['subject_id','charttime','gender','anchor_age','chronic_disease','race','insurance','itemid','valuenum','label_chart']].head())  # Display the first few rows with the desired columns

# Get the total number of rows and columns in the DataFrame
total_rows, total_columns = merged_data_with_labitems.shape
print(f"Total number of rows: {total_rows}")
print(f"Total number of columns: {total_columns}")

# Save the dataframe to a CSV file with a desired name
merged_data_with_labitems.to_csv('merged_data_with_labitems.csv', index=False)

"""# 1. Patients Health Overview Dashboard

1Ô∏è‚É£ Health Status Overview Dashboard
"""

import ipywidgets as widgets
from IPython.display import display, HTML

def assess_health_status(blood_sugar=None, systolic_bp=None, diastolic_bp=None, heart_rate=None, spo2=None):
    status = "Normal"
    color = "green"
    explanation = ""

    if blood_sugar is not None:
        if blood_sugar > 180:
            status, color = "Diabetes (Critical)", "red"
            explanation += "‚ö†Ô∏è **Blood Sugar is critically high. Possible Diabetes.** Seek medical attention.<br>"
        elif blood_sugar > 140:
            status, color = "Pre-Diabetes (At Risk)", "yellow"
            explanation += "‚ö†Ô∏è **Blood Sugar is elevated. Risk of Diabetes.** Monitor diet and exercise.<br>"
        elif blood_sugar < 70:
            status, color = "Hypoglycemia (Critical)", "blue"
            explanation += "‚ö†Ô∏è **Blood Sugar is too low! Risk of Hypoglycemia.** Eat something sugary immediately.<br>"
        else:
            explanation += "‚úÖ **Blood Sugar is in the normal range.**<br>"

    if systolic_bp is not None and diastolic_bp is not None:
        if systolic_bp > 160 or diastolic_bp > 100:
            status, color = "Hypertension (Critical)", "red"
            explanation += "‚ö†Ô∏è **Blood Pressure is dangerously high! Risk of stroke.** See a doctor.<br>"
        elif systolic_bp > 140 or diastolic_bp > 90:
            status, color = "Pre-Hypertension (At Risk)", "yellow"
            explanation += "‚ö†Ô∏è **Blood Pressure is elevated.** Maintain a healthy lifestyle.<br>"
        elif systolic_bp < 90 or diastolic_bp < 60:
            status, color = "Hypotension (Low BP)", "blue"
            explanation += "‚ö†Ô∏è **Blood Pressure is too low.** You may feel dizzy or weak.<br>"
        else:
            explanation += "‚úÖ **Blood Pressure is in the normal range.**<br>"

    if heart_rate is not None:
        if heart_rate > 110:
            status, color = "Tachycardia (Critical)", "red"
            explanation += "‚ö†Ô∏è **Heart Rate is too high! Possible Tachycardia.** Seek medical attention.<br>"
        elif heart_rate < 50:
            status, color = "Bradycardia (Low HR)", "blue"
            explanation += "‚ö†Ô∏è **Heart Rate is too low! Risk of Bradycardia.** Consult a doctor.<br>"
        elif heart_rate > 100:
            status, color = "Elevated Heart Rate (At Risk)", "yellow"
            explanation += "‚ö†Ô∏è **Heart Rate is slightly elevated.** Stay hydrated and rest.<br>"
        else:
            explanation += "‚úÖ **Heart Rate is in the normal range.**<br>"

    if spo2 is not None:
        if spo2 < 90:
            status, color = "Hypoxia (Critical)", "red"
            explanation += "‚ö†Ô∏è **SpO2 is too low! Possible Hypoxia.** Seek emergency care.<br>"
        elif spo2 < 94:
            status, color = "Low Oxygen (At Risk)", "yellow"
            explanation += "‚ö†Ô∏è **SpO2 is slightly low.** Breathe deeply and monitor.<br>"
        else:
            explanation += "‚úÖ **SpO2 is in the normal range.**<br>"

    html = f"""
    <div style='padding: 15px; border-radius: 5px; background-color: {color}; color: white; font-weight: bold; text-align: center; font-size: 16px;'>
        üö® **Health Status: {status}** üö®
    </div>
    <p style='font-size:14px;'>{explanation}</p>
    """
    return HTML(html)

blood_sugar_input = widgets.FloatText(value=88, description="Blood Sugar (mg/dL):", step=1)
systolic_bp_input = widgets.FloatText(value=120, description="Systolic BP:", step=1)
diastolic_bp_input = widgets.FloatText(value=80, description="Diastolic BP:", step=1)
heart_rate_input = widgets.FloatText(value=75, description="Heart Rate:", step=1)
spo2_input = widgets.FloatText(value=98, description="SpO2 (%):", step=1)

def update_dashboard(_):
    display(assess_health_status(
        blood_sugar=blood_sugar_input.value,
        systolic_bp=systolic_bp_input.value,
        diastolic_bp=diastolic_bp_input.value,
        heart_rate=heart_rate_input.value,
        spo2=spo2_input.value
    ))

button = widgets.Button(description="Check Health Status", button_style='info')
button.on_click(update_dashboard)

display(HTML("<h2>üè• **Health Status Overview Dashboard**</h2>"))
display(blood_sugar_input, systolic_bp_input, diastolic_bp_input, heart_rate_input, spo2_input, button)

update_dashboard(None)

"""2Ô∏è‚É£ Patient Vitals Trend Analysis"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta

def generate_vitals_data(days=14):
    np.random.seed(42)
    dates = pd.date_range(end=datetime.today(), periods=days)
    return pd.DataFrame({
        'date': dates,
        'systolic_bp': np.random.randint(110, 160, size=days),
        'diastolic_bp': np.random.randint(70, 100, size=days),
        'heart_rate': np.random.randint(60, 110, size=days),
        'blood_sugar': np.random.randint(80, 200, size=days),
        'pulse': np.random.randint(60, 100, size=days)
    })

def plot_vital_trends(df):
    fig, axes = plt.subplots(3, 2, figsize=(12, 10))
    fig.suptitle("üìä Patient Vitals Trend Analysis", fontsize=16, fontweight="bold")

    vital_signs = {
        "systolic_bp": {"label": "Systolic BP (mmHg)", "color": "blue", "ax": axes[0, 0], "thresholds": (90, 160)},
        "diastolic_bp": {"label": "Diastolic BP (mmHg)", "color": "cyan", "ax": axes[0, 1], "thresholds": (60, 100)},
        "heart_rate": {"label": "Heart Rate (bpm)", "color": "green", "ax": axes[1, 0], "thresholds": (50, 110)},
        "blood_sugar": {"label": "Blood Sugar (mg/dL)", "color": "red", "ax": axes[1, 1], "thresholds": (70, 180)},
        "pulse": {"label": "Pulse (bpm)", "color": "purple", "ax": axes[2, 0], "thresholds": (60, 100)}
    }

    for vital, info in vital_signs.items():
        ax = info["ax"]
        ax.plot(df["date"], df[vital], marker="o", linestyle="-", color=info["color"], label=info["label"])
        ax.fill_between(df["date"], df[vital], color=info["color"], alpha=0.2)
        ax.set_title(info["label"], fontsize=12)
        ax.set_ylabel(info["label"].split("(")[-1].replace(")", ""))
        ax.grid(True)
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=max(len(df)//7, 1)))
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        lower, upper = info["thresholds"]
        abnormal = (df[vital] < lower) | (df[vital] > upper)
        ax.scatter(df["date"][abnormal], df[vital][abnormal], color="black", s=80, edgecolors="yellow", label="‚ö†Ô∏è Abnormal")
        ax.legend()

    axes[2, 1].axis("off")

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()

df = generate_vitals_data(days=14)
plot_vital_trends(df)

"""3Ô∏è‚É£ Medication Adherence Tracking and Visualization"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

def generate_medication_adherence_data(start_date="2024-03-01", end_date="2024-03-31", adherence_rate=0.85):
    np.random.seed(42)
    dates = pd.date_range(start=start_date, end=end_date, freq="D")
    adherence = np.random.choice([1, 0], size=len(dates), p=[adherence_rate, 1 - adherence_rate])

    return pd.DataFrame({"date": dates, "adherence": adherence})

def plot_adherence_bar_chart(df):
    adherence_summary = df["adherence"].value_counts().sort_index()
    adherence_summary.index = ["Missed", "Taken"]

    plt.figure(figsize=(8, 5))
    colors = ["#FF6F61", "#4CAF50"]
    ax = sns.barplot(x=adherence_summary.index, y=adherence_summary.values, palette=colors)

    for i, value in enumerate(adherence_summary.values):
        ax.text(i, value + 1, str(value), ha="center", fontsize=14, fontweight="bold", color="black")

    plt.title(f"üìä Medication Adherence Summary ({df['date'].min().strftime('%B %Y')})", fontsize=14)
    plt.ylabel("Days Count", fontsize=12)
    plt.xlabel("Medication Status", fontsize=12, labelpad=10)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    plt.ylim(0, max(adherence_summary.values) + 5)
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

def plot_adherence_calendar(df):
    df["day"] = df["date"].dt.day
    df["status"] = df["adherence"].map({1: "‚úî", 0: "‚úò"})

    plt.figure(figsize=(12, 2))
    calendar_data = df.pivot_table(index=np.zeros(len(df)), columns="day", values="adherence")

    ax = sns.heatmap(
        calendar_data,
        annot=df["status"].values.reshape(1, -1),
        fmt="",
        cmap=["#FF6F61", "#4CAF50"],
        cbar=False,
        linewidths=2, linecolor="white",
        annot_kws={"size": 12, "color": "black", "weight": "bold"}
    )

    plt.title(f"üìÖ Medication Adherence Calendar ({df['date'].min().strftime('%B %Y')})", fontsize=14, pad=10)
    plt.xticks(ticks=np.arange(0.5, len(df)), labels=df["day"].tolist(), fontsize=10)
    plt.yticks([])
    plt.show()

med_adherence_df = generate_medication_adherence_data(start_date="2024-03-01", end_date="2024-03-31", adherence_rate=0.85)

plot_adherence_bar_chart(med_adherence_df)
plot_adherence_calendar(med_adherence_df)

"""# 2. Disease Prediction and Likelihood Predication Dashboard

Disease Prediction Code
"""

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv("merged_data_with_labitems.csv", low_memory=False)

# Drop unnecessary or leaky columns
df.drop(columns=["subject_id", "charttime", "merged_label", "label_chart", "label_lab"], inplace=True, errors="ignore")

# Ensure 'anchor_year' is properly converted
df['anchor_year'] = df['anchor_year'].astype(str)

# Handling the range case (like '2008 - 2010') by extracting the start year
df['anchor_year'] = df['anchor_year'].apply(lambda x: x.split(' - ')[0] if ' - ' in x else x)

# Convert 'anchor_year' to numeric and handle errors
df['anchor_year'] = pd.to_numeric(df['anchor_year'], errors='coerce')

# Fill NaN values with the median
df['anchor_year'] = df['anchor_year'].fillna(df['anchor_year'].median())

# Encode categorical columns into numeric values
label_encoder = LabelEncoder()

# Identify object (categorical) columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Apply label encoding to categorical columns
for col in categorical_cols:
    df[col] = label_encoder.fit_transform(df[col].astype(str))

# Define target variable and features
y = df["chronic_disease"]  # Target variable
X = df.drop(columns=["chronic_disease"])  # Features

# Convert everything to numeric format
X = X.apply(pd.to_numeric, errors='coerce')

# Fill any remaining NaN values (if present)
X.fillna(X.median(), inplace=True)

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialize RandomForestClassifier with class_weight='balanced'
clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

# Train the model
clf.fit(X_train, y_train)

# Predict on test set
y_pred = clf.predict(X_test)

# Evaluate the model
print("Classification Report:\n", classification_report(y_test, y_pred))

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Use stratified k-fold cross-validation for better evaluation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Get the F1 scores using cross-validation
f1_scores = cross_val_score(clf, X_resampled, y_resampled, cv=cv, scoring='f1_weighted')
print(f"Cross-validated F1 Scores: {f1_scores}")
print(f"Mean Cross-validated F1 Score: {f1_scores.mean():.4f}")

import matplotlib.pyplot as plt
import numpy as np

feature_names = X_train.columns
importances = clf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.bar(range(len(feature_names)), importances[indices], align="center")
plt.xticks(range(len(feature_names)), [feature_names[i] for i in indices], rotation=90)
plt.show()

"""# redo the process"""

import pandas as pd
import numpy as np

# Step 1: Load the data
df = pd.read_csv("merged_data_with_labitems.csv")  # Replace with actual file path

# Step 2: Define the mapping of itemid to vital sign names
vital_signs_mapping = {
    220045: "Heart Rate",
    220179: "Systolic BP",
    220180: "Diastolic BP",
    226537: "Glucose (whole blood)",
    220277: "O2 Saturation"
}

# Step 3: Filter the relevant columns
df_vitals = df[['subject_id', 'charttime', 'gender', 'anchor_age', 'chronic_disease', 'race', 'insurance', 'itemid', 'valuenum']]

# Step 4: Map the itemid to the corresponding vital sign name
df_vitals['vital_sign'] = df_vitals['itemid'].map(vital_signs_mapping)

# Step 5: Handle duplicates by aggregating them (taking the mean)
df_vitals = df_vitals.groupby(["subject_id", "charttime", "gender", "anchor_age", "chronic_disease", "race", "insurance", "vital_sign"], as_index=False).agg({"valuenum": "mean"})

# Step 6: Pivot the DataFrame to make vital signs as columns
df_vitals_pivot = df_vitals.pivot(index=["subject_id", "charttime", "gender", "anchor_age", "chronic_disease", "race", "insurance"],
                                  columns="vital_sign", values="valuenum").reset_index()

# Step 7: Randomly generate Glucose (whole blood) values between 70 and 180 for each row
np.random.seed(42)  # For reproducibility
df_vitals_pivot['Glucose (whole blood)'] = np.random.randint(70, 181, size=len(df_vitals_pivot))

# Step 8: Save the updated dataset with vital sign names
df_vitals_pivot.to_csv("updated_mimic_dataset.csv", index=False)

# Optionally, print a preview of the saved dataset
print(df_vitals_pivot.head())

"""machine learning for disease prediction"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the updated dataset
file_path = "updated_mimic_dataset.csv"  # Ensure the file is in the correct directory
df = pd.read_csv(file_path)

# Display basic information about the dataset
print("Dataset Overview:")
print(df.info())

# Display first 5 rows to preview the data
print("\nPreview of Data:")
print(df.head())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Basic descriptive statistics for numerical columns
print("\nSummary Statistics:")
print(df.describe())

# Check for unique values in categorical columns
categorical_cols = ['gender', 'race', 'insurance', 'chronic_disease']
for col in categorical_cols:
    print(f"\nUnique values in {col}:")
    print(df[col].value_counts())

# Plot distribution of age
plt.figure(figsize=(8, 5))
sns.histplot(df['anchor_age'], bins=30, kde=True, color='skyblue')
plt.title('Age Distribution of Patients')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Plot gender distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='gender', data=df, palette='pastel')
plt.title('Gender Distribution')
plt.xlabel('Gender (0 = Female, 1 = Male)')
plt.ylabel('Count')
plt.show()

# Plot race distribution
plt.figure(figsize=(8, 5))
sns.countplot(y='race', data=df, palette='viridis')
plt.title('Race Distribution')
plt.xlabel('Count')
plt.ylabel('Race')
plt.show()

# Check distribution of chronic diseases
plt.figure(figsize=(8, 5))
sns.countplot(x='chronic_disease', data=df, palette='Set2')
plt.title('Chronic Disease Distribution')
plt.xlabel('Chronic Disease (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.show()

# Correlation heatmap for numerical features
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="YlGnBu", fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Check vital sign distribution (Glucose, BP, HR, O2 Saturation)
vital_signs = ['Glucose (whole blood)', 'Systolic BP', 'Diastolic BP', 'Heart Rate', 'O2 Saturation']

# Plot distribution for each vital sign
for col in vital_signs:
    plt.figure(figsize=(8, 5))
    sns.histplot(df[col], bins=30, kde=True, color='lightcoral')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Analyze correlations between vitals and chronic disease
plt.figure(figsize=(8, 5))
sns.boxplot(x='chronic_disease', y='Glucose (whole blood)', data=df)
plt.title('Glucose Levels vs. Chronic Disease')
plt.xlabel('Chronic Disease (0 = No, 1 = Yes)')
plt.ylabel('Glucose (mg/dL)')
plt.show()

# Group by chronic disease and check mean values
grouped_stats = df.groupby('chronic_disease')[vital_signs + ['anchor_age']].mean().reset_index()
print("\nMean values for vitals by chronic disease:")
print(grouped_stats)

# Save the EDA summary as a CSV file for future reference
grouped_stats.to_csv("eda_summary.csv", index=False)
print("\nEDA summary saved as 'eda_summary.csv'")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

# Step 1: Load the dataset
df = pd.read_csv("updated_mimic_dataset.csv")  # Make sure the path is correct


# Encode categorical variables (e.g., 'gender', 'chronic_disease', 'race', 'insurance')
label_encoder = LabelEncoder()
df['gender'] = label_encoder.fit_transform(df['gender'])
df['chronic_disease'] = label_encoder.fit_transform(df['chronic_disease'])
df['race'] = label_encoder.fit_transform(df['race'])
df['insurance'] = label_encoder.fit_transform(df['insurance'])

# Step 3: Define features (X) and target (y)
# You can choose which columns to include in your features
features = ['gender', 'anchor_age', 'race', 'Diastolic BP', 'Heart Rate', 'O2 Saturation', 'Systolic BP', 'Glucose (whole blood)']
X = df[features]

# Let's assume 'chronic_disease' is the target variable for prediction
y = df['chronic_disease']

# Step 4: Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Initialize and train the model (Random Forest Classifier)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 6: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Detailed classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

""" Feature Importance"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Step 6: Feature importance
importances = model.feature_importances_

# Step 7: Create a DataFrame to hold feature names and their importance
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
})

# Step 8: Sort features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Step 9: Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importance for Disease Prediction')
plt.show()

# Optionally, print the feature importance table
print(feature_importance_df)

"""machine learning for disease progression"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Step 1: Load the updated dataset
df = pd.read_csv("updated_mimic_dataset.csv")


from sklearn.preprocessing import LabelEncoder

# Step 2: Encode categorical variables
# Encode 'gender' using LabelEncoder
le_gender = LabelEncoder()
df['gender_encoded'] = le_gender.fit_transform(df['gender'])

# For 'race', we can use LabelEncoder or One-Hot Encoding
le_race = LabelEncoder()
df['race_encoded'] = le_race.fit_transform(df['race'])

# Step 2: Feature engineering for disease progression prediction
# Create features reflecting change over time for all vitals
df['heart_rate_change'] = df.groupby('subject_id')['Heart Rate'].diff()
df['systolic_bp_change'] = df.groupby('subject_id')['Systolic BP'].diff()
df['diastolic_bp_change'] = df.groupby('subject_id')['Diastolic BP'].diff()
df['o2_saturation_change'] = df.groupby('subject_id')['O2 Saturation'].diff()
df['glucose_change'] = df.groupby('subject_id')['Glucose (whole blood)'].diff()

# Create binary features for blood pressure increase/decrease
df['systolic_bp_increase'] = np.where(df['systolic_bp_change'] > 0, 1, 0)  # 1 if systolic BP increased, 0 otherwise
df['diastolic_bp_increase'] = np.where(df['diastolic_bp_change'] > 0, 1, 0)  # 1 if diastolic BP increased, 0 otherwise

df['systolic_bp_decrease'] = np.where(df['systolic_bp_change'] < 0, 1, 0)  # 1 if systolic BP decreased, 0 otherwise
df['diastolic_bp_decrease'] = np.where(df['diastolic_bp_change'] < 0, 1, 0)  # 1 if diastolic BP decreased, 0 otherwise

# Create disease progression target: For simplicity, we define it based on whether the glucose level increased
df['disease_progression'] = np.where(df['glucose_change'] > 0, 1, 0)  # 1 = progression, 0 = stable


# Step 3: Prepare the features and target variable
features = ['anchor_age', 'gender_encoded', 'race_encoded', 'Heart Rate', 'Systolic BP',
            'Diastolic BP', 'O2 Saturation', 'Glucose (whole blood)', 'heart_rate_change',
            'systolic_bp_change', 'diastolic_bp_change', 'o2_saturation_change',
            'glucose_change', 'systolic_bp_increase', 'diastolic_bp_increase',
            'systolic_bp_decrease', 'diastolic_bp_decrease']

X = df[features]
y = df['disease_progression']  # Disease progression as the target variable

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train a Random Forest model for disease progression prediction
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 6: Evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Optionally, show feature importance for progression prediction
feature_importance = model.feature_importances_
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importance
}).sort_values(by='Importance', ascending=False)

print(importance_df)

"""dashboard"""

!pip install gradio
# Import necessary libraries
import pandas as pd
import numpy as np
import gradio as gr
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import joblib

# Step 1: Load the dataset
df = pd.read_csv("updated_mimic_dataset.csv")  # Make sure the file is uploaded correctly

# Step 2: Encode categorical variables using LabelEncoder
le_gender = LabelEncoder()
df['gender_encoded'] = le_gender.fit_transform(df['gender'])

le_race = LabelEncoder()
df['race_encoded'] = le_race.fit_transform(df['race'])

le_disease = LabelEncoder()
df['chronic_disease_encoded'] = le_disease.fit_transform(df['chronic_disease'])

# Step 3: Define features and target variable
features = ['anchor_age', 'gender_encoded', 'race_encoded', 'Diastolic BP', 'Heart Rate', 'O2 Saturation', 'Systolic BP', 'Glucose (whole blood)']
X = df[features]
y = df['chronic_disease_encoded']

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 6: Save the model
joblib.dump(model, "disease_prediction_model.pkl")

# Step 7: Create a mapping from encoded disease to actual names
disease_mapping = dict(enumerate(le_disease.classes_))

# Step 8: Define the prediction function
def predict_disease(gender, age, race, diastolic_bp, heart_rate, o2_saturation, systolic_bp, glucose):
    try:
        # Encode user inputs
        gender_encoded = le_gender.transform([gender])[0]
        race_encoded = le_race.transform([race])[0]

        # Prepare input data for prediction
        input_data = np.array([[age, gender_encoded, race_encoded, diastolic_bp, heart_rate, o2_saturation, systolic_bp, glucose]])

        # Load the trained model and make predictions
        model = joblib.load("disease_prediction_model.pkl")
        prediction_encoded = model.predict(input_data)[0]

        # Map the predicted value to disease name
        predicted_disease = disease_mapping.get(prediction_encoded, "Unknown Disease")

        return f"Predicted Disease: {predicted_disease}"

    except Exception as e:
        return f"Error: {str(e)}"

# Step 9: Create the Gradio Interface
with gr.Blocks() as app:
    gr.Markdown("## üß¨ Chronic Disease Prediction Dashboard")
    gr.Markdown("Enter patient details to predict the likelihood of having a chronic disease.")

    with gr.Row():
        with gr.Column():
            gender_input = gr.Dropdown(label="Gender", choices=list(le_gender.classes_), type="value")
            age_input = gr.Number(label="Age", value=30)
            race_input = gr.Dropdown(label="Race", choices=list(le_race.classes_), type="value")
            diastolic_bp_input = gr.Number(label="Diastolic BP", value=80)
            heart_rate_input = gr.Number(label="Heart Rate", value=72)
            o2_saturation_input = gr.Number(label="O2 Saturation", value=98)
            systolic_bp_input = gr.Number(label="Systolic BP", value=120)
            glucose_input = gr.Number(label="Glucose (whole blood)", value=90)

        with gr.Column():
            prediction_output = gr.Textbox(label="Prediction Result")
            predict_button = gr.Button("Predict")

    # Step 10: Link the button with prediction function
    predict_button.click(
        predict_disease,
        inputs=[
            gender_input, age_input, race_input,
            diastolic_bp_input, heart_rate_input,
            o2_saturation_input, systolic_bp_input, glucose_input
        ],
        outputs=prediction_output
    )

# Step 11: Launch the app
app.launch(share=True)

"""# Disease Likelihood Predication"""

# Import necessary libraries
import pandas as pd
import numpy as np
import gradio as gr
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.calibration import CalibratedClassifierCV
import joblib

# Step 1: Load the dataset
df = pd.read_csv("updated_mimic_dataset.csv")  # Make sure the path is correct

# Step 2: Encode categorical variables using LabelEncoder
le_gender = LabelEncoder()
df['gender_encoded'] = le_gender.fit_transform(df['gender'])

le_race = LabelEncoder()
df['race_encoded'] = le_race.fit_transform(df['race'])

le_disease = LabelEncoder()
df['chronic_disease_encoded'] = le_disease.fit_transform(df['chronic_disease'])

# Step 3: Define features and target variable
features = ['anchor_age', 'gender_encoded', 'race_encoded', 'Diastolic BP', 'Heart Rate',
            'O2 Saturation', 'Systolic BP', 'Glucose (whole blood)']
X = df[features]
y = df['chronic_disease_encoded']

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Initialize and train the Random Forest model with calibrated probability
model = RandomForestClassifier(n_estimators=100, random_state=42)
calibrated_model = CalibratedClassifierCV(model, method='sigmoid')
calibrated_model.fit(X_train, y_train)

# Step 6: Save the trained model
joblib.dump(calibrated_model, "disease_likelihood_model.pkl")

# Step 7: Create a mapping from encoded disease to actual disease names
disease_mapping = dict(enumerate(le_disease.classes_))

# Step 8: Define the prediction function for likelihood
def predict_disease_likelihood(gender, age, race, diastolic_bp, heart_rate, o2_saturation, systolic_bp, glucose):
    try:
        # Encode user inputs
        gender_encoded = le_gender.transform([gender])[0]
        race_encoded = le_race.transform([race])[0]

        # Prepare input data for prediction
        input_data = np.array([[age, gender_encoded, race_encoded, diastolic_bp, heart_rate, o2_saturation, systolic_bp, glucose]])

        # Load the trained model and predict probabilities
        model = joblib.load("disease_likelihood_model.pkl")
        probabilities = model.predict_proba(input_data)[0]

        # Map disease names to probabilities
        results = {disease_mapping[idx]: f"{(prob * 100):.2f}%" for idx, prob in enumerate(probabilities)}
        sorted_results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))

        return sorted_results

    except Exception as e:
        return {"Error": str(e)}

# Step 9: Create the Gradio Interface
with gr.Blocks() as app:
    gr.Markdown("## üîç Chronic Disease Likelihood Prediction Dashboard")
    gr.Markdown("Enter patient details to predict the likelihood of having chronic diseases.")

    with gr.Row():
        with gr.Column():
            gender_input = gr.Dropdown(label="Gender", choices=list(le_gender.classes_), type="value")
            age_input = gr.Number(label="Age", value=30)
            race_input = gr.Dropdown(label="Race", choices=list(le_race.classes_), type="value")
            diastolic_bp_input = gr.Number(label="Diastolic BP", value=80)
            heart_rate_input = gr.Number(label="Heart Rate", value=72)
            o2_saturation_input = gr.Number(label="O2 Saturation", value=98)
            systolic_bp_input = gr.Number(label="Systolic BP", value=120)
            glucose_input = gr.Number(label="Glucose (whole blood)", value=90)

        with gr.Column():
            prediction_output = gr.JSON(label="Disease Likelihoods")
            predict_button = gr.Button("Predict Likelihood")

    # Step 10: Link button with prediction function
    predict_button.click(
        predict_disease_likelihood,
        inputs=[
            gender_input, age_input, race_input,
            diastolic_bp_input, heart_rate_input,
            o2_saturation_input, systolic_bp_input, glucose_input
        ],
        outputs=prediction_output
    )

# Step 11: Launch the Gradio app
app.launch(share=True)

"""# 3. Cost Estimation Dashboard"""

